{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/shahir/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/datasets/reuters.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/shahir/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/datasets/reuters.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "import numpy as np\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "#top 10000 most frequently used words for each data\n",
    "# so the column number is maximum 10000\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data decoding \n",
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n",
    "train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation \n",
    "\n",
    "#vectorizing the data\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "#vectorizing the labels with one-hot encoding \n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 23ms/step - loss: 3.1411 - accuracy: 0.4120 - val_loss: 1.7617 - val_accuracy: 0.6540\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.5315 - accuracy: 0.6981 - val_loss: 1.3112 - val_accuracy: 0.7270\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0903 - accuracy: 0.7709 - val_loss: 1.1555 - val_accuracy: 0.7620\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.8473 - accuracy: 0.8245 - val_loss: 1.0339 - val_accuracy: 0.7850\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6739 - accuracy: 0.8605 - val_loss: 0.9890 - val_accuracy: 0.7790\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.5182 - accuracy: 0.8920 - val_loss: 0.9315 - val_accuracy: 0.8000\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.4223 - accuracy: 0.9118 - val_loss: 0.8992 - val_accuracy: 0.8110\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.3368 - accuracy: 0.9331 - val_loss: 0.8980 - val_accuracy: 0.8120\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.2741 - accuracy: 0.9412 - val_loss: 0.8812 - val_accuracy: 0.8120\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.2297 - accuracy: 0.9517 - val_loss: 0.9164 - val_accuracy: 0.8140\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1980 - accuracy: 0.9498 - val_loss: 0.9235 - val_accuracy: 0.8090\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1745 - accuracy: 0.9568 - val_loss: 0.9266 - val_accuracy: 0.8130\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1550 - accuracy: 0.9554 - val_loss: 0.9524 - val_accuracy: 0.8120\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.1409 - accuracy: 0.9595 - val_loss: 0.9466 - val_accuracy: 0.8110\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1311 - accuracy: 0.9567 - val_loss: 1.0374 - val_accuracy: 0.7990\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1261 - accuracy: 0.9578 - val_loss: 1.0040 - val_accuracy: 0.8090\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1037 - accuracy: 0.9635 - val_loss: 1.0105 - val_accuracy: 0.8070\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.1011 - accuracy: 0.9629 - val_loss: 1.0030 - val_accuracy: 0.8150\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1070 - accuracy: 0.9599 - val_loss: 1.0405 - val_accuracy: 0.8050\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0904 - accuracy: 0.9661 - val_loss: 1.0553 - val_accuracy: 0.7970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f56c8e77898>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model creation \n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'v_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1dcc1a946f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhistory_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvalildation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'v_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mephocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'v_loss'"
     ]
    }
   ],
   "source": [
    "#plotting the values \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "v_loss = history_dict['v_loss']\n",
    "loss = history_dict['loss']\n",
    "ephocs = range(0, 20)\n",
    "plt.plot(epochs, loss, 'b', labels='Training loss')\n",
    "plt.plot(epochs, validationss, 'bo', labels='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
